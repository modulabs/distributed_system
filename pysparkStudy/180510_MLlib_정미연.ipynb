{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. MLLib\n",
    "=============\n",
    "\n",
    "* * *\n",
    "\n",
    "#### 작성 : 데이터랩 3반 정미연\n",
    "#### 코드 출처는 여기  [위키북스](https://github.com/wikibook/spark/tree/master/Python/ch8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 개요\n",
    "- 머신러닝 : 어떤 주어진 문제를 해결할 때, 필요한 프로그램을 일일이 작성하지 않더라도 *기계 스스로* 데이터를 이용해 문제를 해결할 수 있는 알고리즘을 만드는 것\n",
    "- 회귀, 분류, 그룹화, 추천 등 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 관측과 특성\n",
    "- feature는? 관측 데이터의 속성을 나타내는 것 (변수), 그렇지만 목표에 따라 최종 특성은 달라질 수 있음\n",
    "- 특징 추출이 핵심 활동 중 하나임. 업무 도메인 지식도 필요\n",
    "- 데이터의 변환, 필터링, 정규화, 특성 간 상관관계 분석 등을 포함함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 레이블\n",
    "- 올바른 출력값을 알려주는 값을 *레이블*이라고 함\n",
    "- 지도학습 : 입력에 대한 올바른 출력 값을 알고 있는 데이터셋에서의 입,출력을 학습하고 답이 알려지지 않은 새로운 입력값에 대한 출력값을 찾게 함 (레이블이 있음)\n",
    "- 스파크에서는 LabeledPoint라는 데이터 타입을 사용하여 레이블이 달린 데이터셋을 핸들링함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 연속형 데이터와 이산형 데이터\n",
    "- 연속형 데이터(Continuous) : 연속적인 값을 가진 것 (무게, 온도, 습도 등)\n",
    "- 이산형 데이터(Discrete data) : 불연속적인 값을 가진 것(셀 수 있음)(나이, 성별, 개수 등)\n",
    "\n",
    "- 연속형 데이터는 *실수*값, 이산형 데이터는 *정수,문자*값을 갖는데 스파크에서는 *double*타입의 데이터만 사용할 수 있음. 대신에 이를 활용할 수 있는 다양한 알고리즘을 구현한 API를 제공"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5 알고리즘과 모델\n",
    "- 알고리즘 : 우리가 알고있는 그런 알고리즘들..(로지스틱 회귀, SVM, NN..)\n",
    "- 모델 : 알고리즘의 산출물임. 알고리즘에 데이터를 적용한 것.\n",
    "- 알고리즘에서 모델을 만드는 과정은 많은 자원들을 필요로 하지만 만들고 나서는 입력만 하면 결과를 빨리 얻는다! 스파크에서도 여러가지 기능 제공 중"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.6 파라메트릭 알고리즘\n",
    "- 머신러닝의 궁극적인 목적은 입력 데이터로부터 원하는 출력값을 얻는 것임.\n",
    "\n",
    "- 모수적(Parametric) 알고리즘\n",
    "  - 고정된 개수의 파라미터, 계수를 사용하는 것으로 입력과 출력 사이의 관계를 특성 값에 관한 수학적 함수 또는 수식으로 가정하고, 수식의 결과가 실제 결괏값에 가깝도록 계수륵 조정하는 방법\n",
    "  - Y = a0 + a1X + e 에서 실제값-예측값을 나타내는 손실함수를 정의하여 이 함수를 최소화하는 a0, a1값을 찾아내는 것을 생각해 보자.\n",
    "  - 선형회귀, 로지스틱 회귀가 대표적임  \n",
    "\n",
    "- 비모수적(Non-parametric) 알고리즘\n",
    "  - 입력과 출력 사이의 가설이 없이 수행 결과를 그대로 사용\n",
    "  - 서포트 벡터 머신, 나이브 베이즈 등"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.7 지도학습과 비지도학습\n",
    "\n",
    "입력 데이터의 타입 또는 예측을 수행하는 방식에 따라 학습을 나누었음\n",
    "* * *\n",
    "- 지도학습\n",
    "  - 훈련 데이터에 레이블(정답)이 포함되어 있고 입출력 가설과 레입르을 이용해 오차를 계산하고 입출력 관계를 유추\n",
    "  - 일반적으로 회귀, 분류 등.\n",
    "  - 훈련 데이터에 레이블 꼭 있어야 함\n",
    "  \n",
    "- 비지도학습 \n",
    "  - 특성과 레이블 간의 인과 관계를 모르거나, 지정해두지 않음\n",
    "  - 군집 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.8 훈련 데이터와 테스트 데이터\n",
    "\n",
    "- 지도 학습에서는 데이터를 트레이닝(훈련)/ 테스트 데이터로 나누어서 작업\n",
    "  - 트레이닝 : 모델 생성용\n",
    "  - 테스트 : 모델 성능 측정용\n",
    "  - 트레이닝 셋과 테스트 셋을 명확히 구분해야 하며, 교차검증(데이터셋을 토막내서 테스트셋, 트레이닝셋 토막을 바꿔가며 진행) 등의 방법으로도 효율을 높임\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.9 MLlib API\n",
    "#### 제공 API\n",
    "1. 머신러닝 알고리즘 : 분류, 회귀, 클러스터링, 협업 필터링 등 제공\n",
    "2. 특성 추출, 변환, 선택 AP 제공\n",
    "3. 파이프라인 : 순차적으로 수행할 수 있는 파이프라인 API 제공\n",
    "4. 저장 : 알고리즘, 모델, 파이프라인에 대한 저장 및 불러오기 기능\n",
    "5. 유틸리티 : 선형대수, 통계, 데이터 처리 등 함수 제공"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.10 의존성 설정\n",
    "\n",
    "- 스파크 MLlib을 사용하기 위한 **의존성 설정**이 필요\n",
    "\n",
    "###### 의존성이란?\n",
    "- 코드에서 두 모듈 간의 연결.\n",
    "- 객체지향언어에서는 두 클래스 간의 관계라고도 말함.\n",
    "- 일반적으로 둘 중 하나가 다른 하나를 어떤 용도를 위해 사용함.\n",
    "\n",
    "출처: <http://tony-programming.tistory.com/entry/Dependency-의존성-이란> [Tony Programming]\n",
    "\n",
    "- 스파크는 선형대수 라이브러리 breeze를 사용하는데, 처리 속도 향상을 위해 netlib-java에 의존성이 있어 아나콘다가 없을 시 운영체제에 맞게 따로 설치해야 함\n",
    "- 참고 깃헙은 [여기](http://www.github.com/fommil/netlib-java)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.11 벡터와 LabeledPoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.11.1 벡터\n",
    "- 벡터는 double 타입의 값들을 포함하는 컬렉션으로 구현\n",
    "- 스파크 mllib의 vector는 org.apache.spark.ml.linalg 패키지에 정의된 트레이트(자바의 인터페이스와 유사, 직접 인스턴스를 생성할 수 없음)\n",
    "\n",
    "#### 생성 방법\n",
    "- DenseVector 클래스(값에 대한 정보만 있음)는 dense()\n",
    "- SparseVector(값과 값에 대한 인덱스 정보 포함)는 sparse(), 데이터에 0이 많을 경우 이쪽을 생성하는 것이 좋음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.1  0.   0.2  0.3]\n",
      "[0.1,0.0,0.2,0.3]\n",
      "[ 0.1  0.   0.2  0.3]\n",
      "(4,[0,2,3],[0.1,0.2,0.3])\n",
      "  (0, 0)\t0.1\n",
      "  (2, 0)\t0.2\n",
      "  (3, 0)\t0.3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sps\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "# dense vector\n",
    "v1 = np.array([0.1, 0.0, 0.2, 0.3])\n",
    "v2 = Vectors.dense([0.1, 0.0, 0.2, 0.3])\n",
    "\n",
    "\n",
    "# sparse vector\n",
    "v3 = Vectors.sparse(4, [(0, 0.1), (2, 0.2), (3, 0.3)])\n",
    "v4 = Vectors.sparse(4, [0,2,3], [0.1, 0.2, 0.3])\n",
    "v5 = sps.csc_matrix((np.array([0.1, 0.2, 0.3]), np.array([0,2,3]), np.array([0,3])),shape = (4,1))\n",
    "\n",
    "print(v1)\n",
    "print(v2)\n",
    "print(v3.toArray())\n",
    "print(v4)\n",
    "print(v5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- dense() 메서드\n",
    "  - 벡터에 포함할 데이터를 직접 인자로 전달, 배열을 사용해 한번에 전달\n",
    "  \n",
    "  \n",
    "- sparse() 메서드\n",
    "  - 첫번째 인자 : 전체 벡터의 크기를 지정\n",
    "  - 두번째 인자 : 0이 아닌 데이터의 위치를 나타내는 인덱스 배열 전달\n",
    "  - 마지막 인자 : 각 인덱스 위치에 해당하는 데이터를 전달"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.11.2 LabeledPoint\n",
    "- 레이블을 사용하는 경우를 위한 벡터.특성 값+레이블 포함\n",
    "- double 타입만 할당함.\n",
    "- 머신러닝 작업에서는 sparsevector를 많이 쓰기 때문에 특정 포맷을 가진 텍스트 파일로부터 이 벡터를 생성하는 유틸리티 함수를 제공함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label:1.0, features:[0.1,0.0,0.2,0.3]\n"
     ]
    }
   ],
   "source": [
    "# LabeledPoint 생성\n",
    "# 아래 코드 주석을 지우면 sparsevector 사용이 가능합니다.\n",
    "\n",
    "## Code start\n",
    "import numpy as np\n",
    "# import scipy.sparse as sps\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "#from pyspark.mllib.util import MLUtils\n",
    "#from pyspark.sql import SparkSession\n",
    "\n",
    "#path=\"your spark dir/sample_libsvm_data.txt\"\n",
    "#v7 = MLUtils.loadLibSVMFile(spark.sparkContext, path)\n",
    "#lp1 = v7.first\n",
    "#print(\"label:%s, features:%s\" % (lp1.label, lp1.features))\n",
    "\n",
    "v1 = np.array([0.1, 0.0, 0.2, 0.3])\n",
    "v6 = LabeledPoint(1.0, v1)\n",
    "print(\"label:%s, features:%s\" % (v6.label, v6.features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.12 파이프라인\n",
    "- 데이터 수집, 가공, 특성 추출, 알고리즘 적용 및 모델 생성, 평가, 배포 활용의 작업의 처리 작업을 통함\n",
    "\n",
    "- 파이프라인은 여러 종류의 알고리즘을 *순차적으로 실행할 수 있게* 지원해줌."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- 트랜스포머 transform()\n",
    "  - org.apache.spark.ml 패키지에 선언된 추상 클래스인 Transformer 클래스를 상속하는 클래스\n",
    "  - 데이터 프레임을 변형해서 새로운 데이터 프레임을 생성\n",
    "\n",
    "- 평가자 fit()\n",
    "  - Estimator 추상 클래스를 상속하는 클래스\n",
    "  - 데이터프레임에 알고리즘을 적용해 새로운 트랜스포머를 생성\n",
    "  - 머신러닝 알고리즘도 트랜스포머를 생성하므로 일종의 평가자라고 볼 수 있음\n",
    "  \n",
    "- 파이프라인\n",
    "  - 여러 알고리즘을 순차적으로 실행할 수 있는 워크플로우를 생성하는 평가자\n",
    "  - 하나의 파이프라인은 여러개의 파이프라인 스테이지로 구성되어 있고 순차적으로 실행됨\n",
    "  - 다수의 프랜스포머+평가자의 선형 조합\n",
    "  \n",
    "- ParamMap\n",
    "  - 평가자나 트랜스포머에 파라미터를 전달하기 위한 목적으로 사용되는 클래스\n",
    "  - ex) ParamMap(e1.maxIter -> 10, e2.maxIter -> 20)\n",
    "     - e# : 평가자, maxIter : 함수, 각각 10과 20을 전달하고 싶다는 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---+------+\n",
      "|height|weight|age|gender|\n",
      "+------+------+---+------+\n",
      "|161.0 |69.87 |29 |1.0   |\n",
      "|176.78|74.35 |34 |1.0   |\n",
      "|159.23|58.32 |29 |0.0   |\n",
      "+------+------+---+------+\n",
      "\n",
      "+------+------+---+------+-------------------+\n",
      "|height|weight|age|gender|features           |\n",
      "+------+------+---+------+-------------------+\n",
      "|161.0 |69.87 |29 |1.0   |[161.0,69.87,29.0] |\n",
      "|176.78|74.35 |34 |1.0   |[176.78,74.35,34.0]|\n",
      "|159.23|58.32 |29 |0.0   |[159.23,58.32,29.0]|\n",
      "+------+------+---+------+-------------------+\n",
      "\n",
      "+------+------+---+------+-------------------+--------------------+--------------------+----------+\n",
      "|height|weight|age|gender|           features|       rawPrediction|         probability|prediction|\n",
      "+------+------+---+------+-------------------+--------------------+--------------------+----------+\n",
      "| 161.0| 69.87| 29|   1.0| [161.0,69.87,29.0]|[-3.1955416045617...|[0.03933384524658...|       1.0|\n",
      "|176.78| 74.35| 34|   1.0|[176.78,74.35,34.0]|[-2.5038902805761...|[0.07558590649750...|       1.0|\n",
      "|159.23| 58.32| 29|   0.0|[159.23,58.32,29.0]|[1.91823378447630...|[0.87194134822519...|       0.0|\n",
      "+------+------+---+------+-------------------+--------------------+--------------------+----------+\n",
      "\n",
      "+------+------+---+------+-------------------+--------------------+--------------------+----------+\n",
      "|height|weight|age|gender|           features|       rawPrediction|         probability|prediction|\n",
      "+------+------+---+------+-------------------+--------------------+--------------------+----------+\n",
      "| 161.0| 69.87| 29|   1.0| [161.0,69.87,29.0]|[-3.1955416045615...|[0.03933384524659...|       1.0|\n",
      "|176.78| 74.35| 34|   1.0|[176.78,74.35,34.0]|[-2.5038902805762...|[0.07558590649749...|       1.0|\n",
      "|159.23| 58.32| 29|   0.0|[159.23,58.32,29.0]|[1.91823378447633...|[0.87194134822520...|       0.0|\n",
      "+------+------+---+------+-------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method SparkSession.stop of <pyspark.sql.session.SparkSession object at 0x11143f358>>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 파이프라인 API를 이용한 회귀\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "from pyspark.ml.pipeline import PipelineModel\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"pipeline_sample\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 훈련용 데이터 (키, 몸무게, 나이, 성별)\n",
    "training = spark.createDataFrame([\n",
    "    (161.0, 69.87, 29, 1.0),\n",
    "    (176.78, 74.35, 34, 1.0),\n",
    "    (159.23, 58.32, 29, 0.0)]).toDF(\"height\", \"weight\", \"age\", \"gender\")\n",
    "\n",
    "training.cache()\n",
    "\n",
    "# 테스트용 데이터\n",
    "test = spark.createDataFrame([\n",
    "    (169.4, 75.3, 42),\n",
    "    (185.1, 85.0, 37),\n",
    "    (161.6, 61.2, 28)]).toDF(\"height\", \"weight\", \"age\")\n",
    "\n",
    "training.show(truncate=False)\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"height\", \"weight\", \"age\"], outputCol=\"features\")\n",
    "\n",
    "# training 데이터에 features 컬럼 추가\n",
    "assembled_training = assembler.transform(training)\n",
    "\n",
    "assembled_training.show(truncate=False)\n",
    "\n",
    "# 모델 생성 알고리즘 (로지스틱 회귀 평가자)\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01, labelCol=\"gender\")\n",
    "\n",
    "# 모델 생성\n",
    "model = lr.fit(assembled_training)\n",
    "\n",
    "# 예측값 생성\n",
    "model.transform(assembled_training).show()\n",
    "\n",
    "# 파이프라인\n",
    "pipeline = Pipeline(stages=[assembler, lr])\n",
    "\n",
    "# 파이프라인 모델 생성\n",
    "pipelineModel = pipeline.fit(training)\n",
    "\n",
    "# 파이프라인 모델을 이용한 예측값 생성\n",
    "pipelineModel.transform(training).show()\n",
    "\n",
    "path1 = \"datalab/datalab3/reg-model\"\n",
    "path2 = \"datalab/datalab3/pipeline\"\n",
    "\n",
    "# 모델 저장\n",
    "model.write().overwrite().save(path1)\n",
    "pipelineModel.write().overwrite().save(path2)\n",
    "\n",
    "# 저장된 모델 불러오기\n",
    "loadedModel = LogisticRegressionModel.load(path1)\n",
    "loadedPipelineModel = PipelineModel.load(path2)\n",
    "\n",
    "spark.stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 8.13 알고리즘\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.13.1 Tokenizer\n",
    "- 공백 문자를 기준으로 입력 문자열을 개별 단어의 배열로 변환, 이 배열을 값으로 하는 새로운 칼럼을 생성하는 트랜스포머\n",
    "- 문자열 구분자가 공백이 아니고 정규표현식을 사용할 때는 *RegexTokenizer* 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- input: string (nullable = true)\n",
      " |-- output: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "+---+--------------------+--------------------+\n",
      "| id|               input|              output|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|Tokenization is t...|[tokenization, is...|\n",
      "|  1|Refer to the Toke...|[refer, to, the, ...|\n",
      "+---+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method SparkSession.stop of <pyspark.sql.session.SparkSession object at 0x11143f358>>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"tokenizer_sample\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "data = [(0, \"Tokenization is the process\"), (1, \"Refer to the Tokenizer\")]\n",
    "inputDF = spark.createDataFrame(data).toDF(\"id\", \"input\")\n",
    "tokenizer = Tokenizer(inputCol=\"input\", outputCol=\"output\")\n",
    "outputDF = tokenizer.transform(inputDF)\n",
    "outputDF.printSchema()\n",
    "outputDF.show()\n",
    "\n",
    "spark.stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.13.2 TF-IDF\n",
    "- TF-IDF (Term Frequency - Inverse Document Frequency), 여러 문서들에서 특정 단어가 특정 문서 내에서 가지는 중요도를 수치화한 것\n",
    "  - TF : 단어의 빈도\n",
    "  - IDF : 단어가 나오는 문서의 빈도에 역수 취한 것. 빈도가 높을수록 점수는 낮게 부여된다\n",
    "- 문서에서 출현 빈도가 높은 단어는 일단 가점을 주지만 모든 문서에서 출현 빈도가 낮으면 자주 쓰지만 중요하지 않은 단어로 인지하여 가중치를 다시 낮추게 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+--------------------+\n",
      "|             words|         TF-Features|      Final-Features|\n",
      "+------------------+--------------------+--------------------+\n",
      "|[a, a, a, b, b, c]|(20,[1,2,10],[2.0...|(20,[1,2,10],[0.5...|\n",
      "|         [a, b, c]|(20,[1,2,10],[1.0...|(20,[1,2,10],[0.2...|\n",
      "|   [a, c, a, a, d]|(20,[2,10,14],[1....|(20,[2,10,14],[0....|\n",
      "+------------------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method SparkSession.stop of <pyspark.sql.session.SparkSession object at 0x11143f358>>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import HashingTF\n",
    "from pyspark.ml.feature import IDF\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"tf_idf_sample\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df1 = spark.createDataFrame([\n",
    "    (0, \"a a a b b c\"),\n",
    "    (0, \"a b c\"),\n",
    "    (1, \"a c a a d\")]).toDF(\"label\", \"sentence\")\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "\n",
    "# 각 문장을 단어로 분리\n",
    "df2 = tokenizer.transform(df1)\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"TF-Features\", numFeatures=20)\n",
    "df3 = hashingTF.transform(df2)\n",
    "\n",
    "df3.cache()\n",
    "\n",
    "idf = IDF(inputCol=\"TF-Features\", outputCol=\"Final-Features\")\n",
    "idfModel = idf.fit(df3)\n",
    "\n",
    "rescaledData = idfModel.transform(df3)\n",
    "rescaledData.select(\"words\", \"TF-Features\", \"Final-Features\").show()\n",
    "\n",
    "spark.stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.13.3 StringIndexer, IndexToString\n",
    "\n",
    "- StringIndexer : 문자열 칼럼에 대응하는 숫자형 칼럼을 생성하는 평가자. 해당 칼럼의 모드 문자열에 노출 빈도에 따른 인덱스를 부여해서 숫자로 된 레이블 칼럼을 생성\n",
    "- IndexToString : 스트링인덱서의 인코딩 결과를 원래 문자열을 되돌려주는 트랜스포머. 최종 데이터셋에서 원래의 문자열 레이블로 되돌릴 때"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.14 회귀와 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.14.1 회귀\n",
    "\n",
    "- 회귀분석 : 연속형 데이터 모임에서 독립변수와 종속변수의 모형을 만들어 관계를 찾는 것\n",
    "- 선형회귀, 일반화 선형회귀, 의사결정 트리 회귀, 랜덤 포레스트 회귀, 그레디언트 부스티드 트리 회귀, 생존 회귀, 등위 회귀 등 제공\n",
    "\n",
    "* * *\n",
    "- 선형 회귀: 관측 대상이 갖고 있는 어떤 특성이 그 대상이 가지고 있는 또 다른 특성과 일종의 선형 관계를 가진다는 가정 하에 예측을 수행. 잔차가 정규 분포를 따르는 경우에 사용\n",
    "  - y = x1i + x2i + x3i + i(오차) 라고 식을 만들어 놨을때 실제값과 예측값의 차이(잔차)를 줄이고자 함\n",
    "  - 여기서 손실함수, 정규화(구간을 일치시킴) 등에 대한 공부가 필요\n",
    "  - elasticNetParam, maxIter, reqParam(L1, L2), solver\n",
    "  \n",
    "- 일반화 선형 회귀 : 선형 회귀를 일반화하여 오차 분포가 정규분포를 따르지 않아도 적용이 가능함\n",
    "  - family(확률 분포), link(회귀에서 사용할 선형 함수), maxIter, reqParam, Solver\n",
    "\n",
    "- 의사결정 트리 회귀 : 관측값의 특성 값을 기준으로 노드를 만들어서 루트 노드로부터 분기가 수행되지 않는 리프 노드까지 트리를 구성하여 예측/분류를 하는 모양. 요즘은 앙상블 형태로도 응용\n",
    "  - DecisionTressRegressor 클래스를 제공\n",
    "  - impurity(불순도 측정방법), maxBins, maxDepth, minifoGain, minInstancesPerNode\n",
    "  \n",
    "- 랜덤 포레스트 회귀 : 다수의 의사결정 트리를 결합. 앙상블 기법의 일종\n",
    "  - 일반적으로 의사결정트리는 단순하고 직관적이여서 이해가 쉽지만 학습 데이터에 따라 결과가 크게 달라지고 오버피팅의 위험이 있어 보통 앙상블 기법을 많이 적용함\n",
    "  - 랜덤 포레스트는 데이터에서 임의의 서브 데이터 셋을 추출하고 여러개의 의사결정나무를 심어서 적용해보고 각 나무의 결과를 취합하여 최종 결과를 만들어낸다. 오차를 줄이는 효과를 같이 볼 수 있음. 병렬로 진행\n",
    "  - featureSubsetStrategy, impurity, maxBins, maxDepth, minInfoGain, minInstancesPerNode, numTress, subsamplingRate\n",
    "\n",
    "- 그레디언트 부스티드 트리 회귀 : 앙상블 기법의 일종. 의사결정나무를 순차적으로 처리하여 이터레이션 뒤 손실함수를 확인하여 오류를 수정해나가면서 나무를 심는다. 정확도를 높일수는 있지만 연산이 느림\n",
    "  - Stochastic Gradient Boosting - GBTRegressor\n",
    "  - impurity, lossType, maxBins, maxDepth, maxIter, Stepsize, minInfoGain, minInstancesPerNode, subsamplingRate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.14.2 분류\n",
    "- 분류: 특정 데이터를 사전에 정해진 기준에 따라 카테고리로 분류하는 것. 레이블이 달려 있으므로 지도학습임\n",
    "- 로지스틱 회귀, 의사결정 트리, 랜덤 포레스트, 그레디언트 부스티드 트리, 다중 퍼셉트론,one-vs-rest, 나이브 베이즈 등을 제공\n",
    "* * *\n",
    "- 로지스틱 회귀 : 종속 변수가 카테고리. 일반화 선형 회귀의 일종이나 '분류 모델'임. 종속 변수의 값을 0~1 사이로 돌려주는 시그모이드 함수를 사용해서 그렇게 부름\n",
    "  - elasticNetParam, family, maxIter, reqParam, threshold(s)\n",
    "  \n",
    "- 의사결정 트리 : 의사결정트리 회귀와 같이 나무를 심지만 '분류'에 초점을 둔 것임\n",
    "  - 랜덤 포레스트, 그레디언트 부스티드 트리 등\n",
    "  \n",
    "- 다중 퍼셉트론 분류자 : 신경망에서 은닉층이 있는. 우리가 일반적으로 알고있는 뉴럴 네트워크임. 입력값이 들어가고 은닉층을 통과해서 최종 노드에서 어떤 결과값이 나오게 됨\n",
    "  - 입력층 이외에서는 이전 단계의 노드들을 병합하여 값을 결정하는데 바이어스와 활성화 함수, 가중치가 영향을 미침\n",
    "  - 역전파법. 자세한 사항은 [여기](https://ratsgo.github.io/deep%20learning/2017/05/14/backprop/)\n",
    "  - laters, maxIter, stepSize, featurescol, labelCol\n",
    " \n",
    "- One-vs-Rest Classifer : 다중 클래스 분류. 전체 데이터를 다수의 이항 분포르 나뉨\n",
    "  - n개의 클래스를 가진 데이터셋이 있다면 이 데이터셋을 특정 클래스에 속하는 데이터와 나머지 n-1개의 클래스에 속하는 데이터로 분류하는 n개의 이진 분류자를 생성\n",
    "  \n",
    "- 나이브 베이즈 : 독립 변수의 특성들이 서로 독립이라는 가정 하에 분류를 수행한다. 베이즈 이론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.16 협업 필터링\n",
    "\n",
    "- 사용자의 선호도를 바탕으로 유사한 성향을 가진 사용자를 찾아서 관심사를 예측하는 방법. 추천 시스템이 유명\n",
    "- 사용자와 상품 간 추천정보를 나타내는 Sparse matrix에서 비어있는 값을 찾아내서 작업함\n",
    "- ALS 알고리즘에 근거하여 작업\n",
    "- rank, maxIter, reqParam, implicitPrefs, alpha, numBlocks\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
